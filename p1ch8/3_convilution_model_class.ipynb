{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": "<torch._C.Generator at 0x10555b610>"
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import time\n",
    "\n",
    "torch.set_printoptions(edgeitems=2)\n",
    "# 默认当前数据处理\n",
    "torch.manual_seed(123)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "# 加载数据集\n",
    "transform = transforms.Compose(\n",
    "    [transforms.ToTensor(),\n",
    "     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
    "\n",
    "data_path = '../data-unversioned/p1ch7/'\n",
    "\n",
    "trainset = torchvision.datasets.CIFAR10(root=data_path, train=True,\n",
    "                                        download=True, transform=transform)\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=4,\n",
    "                                          shuffle=True, num_workers=2)\n",
    "\n",
    "testset = torchvision.datasets.CIFAR10(root=data_path, train=False,\n",
    "                                       download=True, transform=transform)\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=4,\n",
    "                                         shuffle=False, num_workers=2)\n",
    "\n",
    "classes = ('plane', 'car', 'bird', 'cat',\n",
    "           'deer', 'dog', 'frog', 'horse', 'ship', 'truck')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "# 定义模型\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 6, 5)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.conv2 = nn.Conv2d(6, 16, 5)\n",
    "        self.fc1 = nn.Linear(16 * 5 * 5, 120)\n",
    "        self.fc2 = nn.Linear(120, 84)\n",
    "        self.fc3 = nn.Linear(84, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(torch.relu(self.conv1(x)))\n",
    "        x = self.pool(torch.relu(self.conv2(x)))\n",
    "        x = x.view(-1, 16 * 5 * 5)\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "data": {
      "text/plain": "(62006, [450, 6, 2400, 16, 48000, 120, 10080, 84, 840, 10])"
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "moedel = Net()\n",
    "numel_list = [p.numel() for p in moedel.parameters()]\n",
    "sum(numel_list), numel_list"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "# import torch.multiprocessing as mp\n",
    "#\n",
    "# # 设置MPS的最大内存限制为4GB\n",
    "# if mp.get_start_method() != 'spawn':\n",
    "#     mp.set_start_method('spawn')\n",
    "# mp.set_sharing_strategy('file_system')\n",
    "# mp.multiprocessing.set_start_method(\"spawn\", force=True)\n",
    "# mp.get_context(\"spawn\").mp_metrics._max_dev_mem_size = 4294967296"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1,  2000] loss: 2.216\n",
      "[1,  4000] loss: 1.874\n",
      "[1,  6000] loss: 1.675\n",
      "[1,  8000] loss: 1.606\n",
      "[1, 10000] loss: 1.548\n",
      "[1, 12000] loss: 1.490\n",
      "[2,  2000] loss: 1.417\n",
      "[2,  4000] loss: 1.392\n",
      "[2,  6000] loss: 1.368\n",
      "[2,  8000] loss: 1.346\n",
      "[2, 10000] loss: 1.323\n",
      "[2, 12000] loss: 1.307\n",
      "[3,  2000] loss: 1.233\n",
      "[3,  4000] loss: 1.225\n",
      "[3,  6000] loss: 1.235\n",
      "[3,  8000] loss: 1.186\n",
      "[3, 10000] loss: 1.209\n",
      "[3, 12000] loss: 1.178\n",
      "[4,  2000] loss: 1.092\n",
      "[4,  4000] loss: 1.122\n",
      "[4,  6000] loss: 1.127\n",
      "[4,  8000] loss: 1.103\n",
      "[4, 10000] loss: 1.097\n",
      "[4, 12000] loss: 1.081\n",
      "[5,  2000] loss: 0.997\n",
      "[5,  4000] loss: 1.001\n",
      "[5,  6000] loss: 1.034\n",
      "[5,  8000] loss: 1.036\n",
      "[5, 10000] loss: 1.021\n",
      "[5, 12000] loss: 1.041\n",
      "[6,  2000] loss: 0.935\n",
      "[6,  4000] loss: 0.952\n",
      "[6,  6000] loss: 0.957\n",
      "[6,  8000] loss: 0.984\n",
      "[6, 10000] loss: 0.963\n",
      "[6, 12000] loss: 0.968\n",
      "[7,  2000] loss: 0.856\n",
      "[7,  4000] loss: 0.911\n",
      "[7,  6000] loss: 0.927\n",
      "[7,  8000] loss: 0.919\n",
      "[7, 10000] loss: 0.947\n",
      "[7, 12000] loss: 0.921\n",
      "[8,  2000] loss: 0.817\n",
      "[8,  4000] loss: 0.879\n",
      "[8,  6000] loss: 0.866\n",
      "[8,  8000] loss: 0.878\n",
      "[8, 10000] loss: 0.879\n",
      "[8, 12000] loss: 0.902\n",
      "[9,  2000] loss: 0.785\n",
      "[9,  4000] loss: 0.821\n",
      "[9,  6000] loss: 0.829\n",
      "[9,  8000] loss: 0.865\n",
      "[9, 10000] loss: 0.861\n",
      "[9, 12000] loss: 0.862\n",
      "[10,  2000] loss: 0.774\n",
      "[10,  4000] loss: 0.809\n",
      "[10,  6000] loss: 0.804\n",
      "[10,  8000] loss: 0.811\n",
      "[10, 10000] loss: 0.823\n",
      "[10, 12000] loss: 0.836\n",
      "[11,  2000] loss: 0.752\n",
      "[11,  4000] loss: 0.744\n",
      "[11,  6000] loss: 0.778\n",
      "[11,  8000] loss: 0.795\n",
      "[11, 10000] loss: 0.815\n",
      "[11, 12000] loss: 0.797\n",
      "[12,  2000] loss: 0.689\n",
      "[12,  4000] loss: 0.743\n",
      "[12,  6000] loss: 0.769\n",
      "[12,  8000] loss: 0.751\n",
      "[12, 10000] loss: 0.770\n",
      "[12, 12000] loss: 0.804\n",
      "[13,  2000] loss: 0.663\n",
      "[13,  4000] loss: 0.703\n",
      "[13,  6000] loss: 0.734\n",
      "[13,  8000] loss: 0.750\n",
      "[13, 10000] loss: 0.780\n",
      "[13, 12000] loss: 0.775\n",
      "[14,  2000] loss: 0.627\n",
      "[14,  4000] loss: 0.698\n",
      "[14,  6000] loss: 0.718\n",
      "[14,  8000] loss: 0.720\n",
      "[14, 10000] loss: 0.743\n",
      "[14, 12000] loss: 0.768\n",
      "[15,  2000] loss: 0.612\n",
      "[15,  4000] loss: 0.675\n",
      "[15,  6000] loss: 0.696\n",
      "[15,  8000] loss: 0.691\n",
      "[15, 10000] loss: 0.732\n",
      "[15, 12000] loss: 0.734\n",
      "[16,  2000] loss: 0.621\n",
      "[16,  4000] loss: 0.641\n",
      "[16,  6000] loss: 0.688\n",
      "[16,  8000] loss: 0.688\n",
      "[16, 10000] loss: 0.736\n",
      "[16, 12000] loss: 0.726\n",
      "[17,  2000] loss: 0.605\n",
      "[17,  4000] loss: 0.646\n",
      "[17,  6000] loss: 0.693\n",
      "[17,  8000] loss: 0.697\n",
      "[17, 10000] loss: 0.697\n",
      "[17, 12000] loss: 0.683\n",
      "[18,  2000] loss: 0.612\n",
      "[18,  4000] loss: 0.630\n",
      "[18,  6000] loss: 0.656\n",
      "[18,  8000] loss: 0.659\n",
      "[18, 10000] loss: 0.704\n",
      "[18, 12000] loss: 0.708\n",
      "[19,  2000] loss: 0.599\n",
      "[19,  4000] loss: 0.621\n",
      "[19,  6000] loss: 0.622\n",
      "[19,  8000] loss: 0.650\n",
      "[19, 10000] loss: 0.653\n",
      "[19, 12000] loss: 0.705\n",
      "[20,  2000] loss: 0.570\n",
      "[20,  4000] loss: 0.593\n",
      "[20,  6000] loss: 0.627\n",
      "[20,  8000] loss: 0.641\n",
      "[20, 10000] loss: 0.701\n",
      "[20, 12000] loss: 0.695\n",
      "[21,  2000] loss: 0.560\n",
      "[21,  4000] loss: 0.603\n",
      "[21,  6000] loss: 0.636\n",
      "[21,  8000] loss: 0.646\n",
      "[21, 10000] loss: 0.670\n",
      "[21, 12000] loss: 0.646\n",
      "[22,  2000] loss: 0.570\n",
      "[22,  4000] loss: 0.593\n",
      "[22,  6000] loss: 0.610\n",
      "[22,  8000] loss: 0.637\n",
      "[22, 10000] loss: 0.631\n",
      "[22, 12000] loss: 0.660\n",
      "[23,  2000] loss: 0.556\n",
      "[23,  4000] loss: 0.570\n",
      "[23,  6000] loss: 0.608\n",
      "[23,  8000] loss: 0.623\n",
      "[23, 10000] loss: 0.640\n",
      "[23, 12000] loss: 0.662\n",
      "[24,  2000] loss: 0.524\n",
      "[24,  4000] loss: 0.573\n",
      "[24,  6000] loss: 0.597\n",
      "[24,  8000] loss: 0.638\n",
      "[24, 10000] loss: 0.640\n",
      "[24, 12000] loss: 0.667\n",
      "[25,  2000] loss: 0.516\n",
      "[25,  4000] loss: 0.581\n",
      "[25,  6000] loss: 0.589\n",
      "[25,  8000] loss: 0.616\n",
      "[25, 10000] loss: 0.611\n",
      "[25, 12000] loss: 0.651\n",
      "[26,  2000] loss: 0.511\n",
      "[26,  4000] loss: 0.590\n",
      "[26,  6000] loss: 0.592\n",
      "[26,  8000] loss: 0.620\n",
      "[26, 10000] loss: 0.648\n",
      "[26, 12000] loss: 0.649\n",
      "[27,  2000] loss: 0.526\n",
      "[27,  4000] loss: 0.548\n",
      "[27,  6000] loss: 0.595\n",
      "[27,  8000] loss: 0.597\n",
      "[27, 10000] loss: 0.641\n",
      "[27, 12000] loss: 0.653\n",
      "[28,  2000] loss: 0.518\n",
      "[28,  4000] loss: 0.581\n",
      "[28,  6000] loss: 0.586\n",
      "[28,  8000] loss: 0.602\n",
      "[28, 10000] loss: 0.636\n",
      "[28, 12000] loss: 0.638\n",
      "[29,  2000] loss: 0.506\n",
      "[29,  4000] loss: 0.562\n",
      "[29,  6000] loss: 0.576\n",
      "[29,  8000] loss: 0.585\n",
      "[29, 10000] loss: 0.618\n",
      "[29, 12000] loss: 0.612\n",
      "[30,  2000] loss: 0.505\n",
      "[30,  4000] loss: 0.544\n",
      "[30,  6000] loss: 0.586\n",
      "[30,  8000] loss: 0.627\n",
      "[30, 10000] loss: 0.586\n",
      "[30, 12000] loss: 0.619\n",
      "[31,  2000] loss: 0.508\n",
      "[31,  4000] loss: 0.552\n",
      "[31,  6000] loss: 0.615\n",
      "[31,  8000] loss: 0.608\n",
      "[31, 10000] loss: 0.601\n",
      "[31, 12000] loss: 0.631\n",
      "[32,  2000] loss: 0.527\n",
      "[32,  4000] loss: 0.537\n",
      "[32,  6000] loss: 0.581\n",
      "[32,  8000] loss: 0.572\n",
      "[32, 10000] loss: 0.616\n",
      "[32, 12000] loss: 0.646\n",
      "[33,  2000] loss: 0.503\n",
      "[33,  4000] loss: 0.578\n",
      "[33,  6000] loss: 0.581\n",
      "[33,  8000] loss: 0.606\n",
      "[33, 10000] loss: 0.610\n",
      "[33, 12000] loss: 0.621\n",
      "[34,  2000] loss: 0.478\n",
      "[34,  4000] loss: 0.534\n",
      "[34,  6000] loss: 0.565\n",
      "[34,  8000] loss: 0.613\n",
      "[34, 10000] loss: 0.611\n",
      "[34, 12000] loss: 0.627\n",
      "[35,  2000] loss: 0.486\n",
      "[35,  4000] loss: 0.548\n",
      "[35,  6000] loss: 0.591\n",
      "[35,  8000] loss: 0.606\n",
      "[35, 10000] loss: 0.608\n",
      "[35, 12000] loss: 0.629\n",
      "[36,  2000] loss: 0.519\n",
      "[36,  4000] loss: 0.511\n",
      "[36,  6000] loss: 0.568\n",
      "[36,  8000] loss: 0.599\n",
      "[36, 10000] loss: 0.637\n",
      "[36, 12000] loss: 0.655\n",
      "[37,  2000] loss: 0.489\n",
      "[37,  4000] loss: 0.539\n",
      "[37,  6000] loss: 0.566\n",
      "[37,  8000] loss: 0.607\n",
      "[37, 10000] loss: 0.652\n",
      "[37, 12000] loss: 0.628\n",
      "[38,  2000] loss: 0.481\n",
      "[38,  4000] loss: 0.573\n",
      "[38,  6000] loss: 0.591\n",
      "[38,  8000] loss: 0.588\n",
      "[38, 10000] loss: 0.653\n",
      "[38, 12000] loss: 0.588\n",
      "[39,  2000] loss: 0.504\n",
      "[39,  4000] loss: 0.541\n",
      "[39,  6000] loss: 0.576\n",
      "[39,  8000] loss: 0.581\n",
      "[39, 10000] loss: 0.591\n",
      "[39, 12000] loss: 0.599\n",
      "[40,  2000] loss: 0.506\n",
      "[40,  4000] loss: 0.518\n",
      "[40,  6000] loss: 0.567\n",
      "[40,  8000] loss: 0.589\n",
      "[40, 10000] loss: 0.642\n",
      "[40, 12000] loss: 0.609\n",
      "[41,  2000] loss: 0.486\n",
      "[41,  4000] loss: 0.530\n",
      "[41,  6000] loss: 0.567\n",
      "[41,  8000] loss: 0.598\n",
      "[41, 10000] loss: 0.601\n",
      "[41, 12000] loss: 0.631\n",
      "[42,  2000] loss: 0.494\n",
      "[42,  4000] loss: 0.554\n",
      "[42,  6000] loss: 0.554\n",
      "[42,  8000] loss: 0.626\n",
      "[42, 10000] loss: 0.628\n",
      "[42, 12000] loss: 0.613\n",
      "[43,  2000] loss: 0.504\n",
      "[43,  4000] loss: 0.558\n",
      "[43,  6000] loss: 0.571\n",
      "[43,  8000] loss: 0.580\n",
      "[43, 10000] loss: 0.600\n",
      "[43, 12000] loss: 0.637\n",
      "[44,  2000] loss: 0.524\n",
      "[44,  4000] loss: 0.537\n",
      "[44,  6000] loss: 0.589\n",
      "[44,  8000] loss: 0.581\n",
      "[44, 10000] loss: 0.607\n",
      "[44, 12000] loss: 0.621\n",
      "[45,  2000] loss: 0.509\n",
      "[45,  4000] loss: 0.543\n",
      "[45,  6000] loss: 0.573\n",
      "[45,  8000] loss: 0.566\n",
      "[45, 10000] loss: 0.593\n",
      "[45, 12000] loss: 0.602\n",
      "[46,  2000] loss: 0.518\n",
      "[46,  4000] loss: 0.519\n",
      "[46,  6000] loss: 0.580\n",
      "[46,  8000] loss: 0.592\n",
      "[46, 10000] loss: 0.613\n",
      "[46, 12000] loss: 0.597\n",
      "[47,  2000] loss: 0.490\n",
      "[47,  4000] loss: 0.525\n",
      "[47,  6000] loss: 0.573\n",
      "[47,  8000] loss: 0.551\n",
      "[47, 10000] loss: 0.590\n",
      "[47, 12000] loss: 0.617\n",
      "[48,  2000] loss: 0.535\n",
      "[48,  4000] loss: 0.556\n",
      "[48,  6000] loss: 0.569\n",
      "[48,  8000] loss: 0.612\n",
      "[48, 10000] loss: 0.606\n",
      "[48, 12000] loss: 0.627\n",
      "[49,  2000] loss: 0.501\n",
      "[49,  4000] loss: 0.536\n",
      "[49,  6000] loss: 0.585\n",
      "[49,  8000] loss: 0.591\n",
      "[49, 10000] loss: 0.601\n",
      "[49, 12000] loss: 0.627\n",
      "[50,  2000] loss: 0.518\n",
      "[50,  4000] loss: 0.529\n",
      "[50,  6000] loss: 0.575\n",
      "[50,  8000] loss: 0.573\n",
      "[50, 10000] loss: 0.624\n",
      "[50, 12000] loss: 0.621\n",
      "[51,  2000] loss: 0.528\n",
      "[51,  4000] loss: 0.574\n",
      "[51,  6000] loss: 0.561\n",
      "[51,  8000] loss: 0.603\n",
      "[51, 10000] loss: 0.590\n",
      "[51, 12000] loss: 0.633\n",
      "[52,  2000] loss: 0.521\n",
      "[52,  4000] loss: 0.531\n",
      "[52,  6000] loss: 0.578\n",
      "[52,  8000] loss: 0.609\n",
      "[52, 10000] loss: 0.568\n",
      "[52, 12000] loss: 0.614\n",
      "[53,  2000] loss: 0.483\n",
      "[53,  4000] loss: 0.558\n",
      "[53,  6000] loss: 0.612\n",
      "[53,  8000] loss: 0.609\n",
      "[53, 10000] loss: 0.600\n",
      "[53, 12000] loss: 0.597\n",
      "[54,  2000] loss: 0.505\n",
      "[54,  4000] loss: 0.582\n",
      "[54,  6000] loss: 0.595\n",
      "[54,  8000] loss: 0.601\n",
      "[54, 10000] loss: 0.614\n",
      "[54, 12000] loss: 0.621\n",
      "[55,  2000] loss: 0.533\n",
      "[55,  4000] loss: 0.535\n",
      "[55,  6000] loss: 0.565\n",
      "[55,  8000] loss: 0.612\n",
      "[55, 10000] loss: 0.615\n",
      "[55, 12000] loss: 0.618\n",
      "[56,  2000] loss: 0.495\n",
      "[56,  4000] loss: 0.510\n",
      "[56,  6000] loss: 0.615\n",
      "[56,  8000] loss: 0.599\n",
      "[56, 10000] loss: 0.628\n",
      "[56, 12000] loss: 0.606\n",
      "[57,  2000] loss: 0.520\n",
      "[57,  4000] loss: 0.572\n",
      "[57,  6000] loss: 0.575\n",
      "[57,  8000] loss: 0.594\n",
      "[57, 10000] loss: 0.609\n",
      "[57, 12000] loss: 0.609\n",
      "[58,  2000] loss: 0.528\n",
      "[58,  4000] loss: 0.534\n",
      "[58,  6000] loss: 0.573\n",
      "[58,  8000] loss: 0.609\n",
      "[58, 10000] loss: 0.610\n",
      "[58, 12000] loss: 0.652\n",
      "[59,  2000] loss: 0.506\n",
      "[59,  4000] loss: 0.576\n",
      "[59,  6000] loss: 0.576\n",
      "[59,  8000] loss: 0.565\n",
      "[59, 10000] loss: 0.597\n",
      "[59, 12000] loss: 0.628\n",
      "[60,  2000] loss: 0.515\n",
      "[60,  4000] loss: 0.538\n",
      "[60,  6000] loss: 0.582\n",
      "[60,  8000] loss: 0.612\n",
      "[60, 10000] loss: 0.623\n",
      "[60, 12000] loss: 0.669\n",
      "[61,  2000] loss: 0.537\n",
      "[61,  4000] loss: 0.566\n",
      "[61,  6000] loss: 0.591\n",
      "[61,  8000] loss: 0.614\n",
      "[61, 10000] loss: 0.653\n",
      "[61, 12000] loss: 0.635\n",
      "[62,  2000] loss: 0.505\n",
      "[62,  4000] loss: 0.563\n",
      "[62,  6000] loss: 0.592\n",
      "[62,  8000] loss: 0.555\n",
      "[62, 10000] loss: 0.648\n",
      "[62, 12000] loss: 0.655\n",
      "[63,  2000] loss: 0.542\n",
      "[63,  4000] loss: 0.576\n",
      "[63,  6000] loss: 0.604\n",
      "[63,  8000] loss: 0.626\n",
      "[63, 10000] loss: 0.637\n",
      "[63, 12000] loss: 0.659\n",
      "[64,  2000] loss: 0.553\n",
      "[64,  4000] loss: 0.564\n",
      "[64,  6000] loss: 0.580\n",
      "[64,  8000] loss: 0.628\n",
      "[64, 10000] loss: 0.616\n",
      "[64, 12000] loss: 0.677\n",
      "[65,  2000] loss: 0.531\n",
      "[65,  4000] loss: 0.541\n",
      "[65,  6000] loss: 0.586\n",
      "[65,  8000] loss: 0.589\n",
      "[65, 10000] loss: 0.602\n",
      "[65, 12000] loss: 0.641\n",
      "[66,  2000] loss: 0.507\n",
      "[66,  4000] loss: 0.555\n",
      "[66,  6000] loss: 0.609\n",
      "[66,  8000] loss: 0.607\n",
      "[66, 10000] loss: 0.637\n",
      "[66, 12000] loss: 0.651\n",
      "[67,  2000] loss: 0.554\n",
      "[67,  4000] loss: 0.544\n",
      "[67,  6000] loss: 0.605\n",
      "[67,  8000] loss: 0.590\n",
      "[67, 10000] loss: 0.630\n",
      "[67, 12000] loss: 0.646\n",
      "[68,  2000] loss: 0.540\n",
      "[68,  4000] loss: 0.548\n",
      "[68,  6000] loss: 0.609\n",
      "[68,  8000] loss: 0.612\n",
      "[68, 10000] loss: 0.608\n",
      "[68, 12000] loss: 0.606\n",
      "[69,  2000] loss: 0.595\n",
      "[69,  4000] loss: 0.571\n",
      "[69,  6000] loss: 0.578\n",
      "[69,  8000] loss: 0.594\n",
      "[69, 10000] loss: 0.616\n",
      "[69, 12000] loss: 0.609\n",
      "[70,  2000] loss: 0.506\n",
      "[70,  4000] loss: 0.585\n",
      "[70,  6000] loss: 0.607\n",
      "[70,  8000] loss: 0.567\n",
      "[70, 10000] loss: 0.615\n",
      "[70, 12000] loss: 0.634\n",
      "[71,  2000] loss: 0.513\n",
      "[71,  4000] loss: 0.582\n",
      "[71,  6000] loss: 0.623\n",
      "[71,  8000] loss: 0.609\n",
      "[71, 10000] loss: 0.632\n",
      "[71, 12000] loss: 0.664\n",
      "[72,  2000] loss: 0.574\n",
      "[72,  4000] loss: 0.552\n",
      "[72,  6000] loss: 0.608\n",
      "[72,  8000] loss: 0.628\n",
      "[72, 10000] loss: 0.620\n",
      "[72, 12000] loss: 0.632\n",
      "[73,  2000] loss: 0.551\n",
      "[73,  4000] loss: 0.579\n",
      "[73,  6000] loss: 0.590\n",
      "[73,  8000] loss: 0.608\n",
      "[73, 10000] loss: 0.652\n",
      "[73, 12000] loss: 0.683\n",
      "[74,  2000] loss: 0.563\n",
      "[74,  4000] loss: 0.573\n",
      "[74,  6000] loss: 0.614\n",
      "[74,  8000] loss: 0.639\n",
      "[74, 10000] loss: 0.661\n",
      "[74, 12000] loss: 0.667\n",
      "[75,  2000] loss: 0.565\n",
      "[75,  4000] loss: 0.585\n",
      "[75,  6000] loss: 0.637\n",
      "[75,  8000] loss: 0.617\n",
      "[75, 10000] loss: 0.607\n",
      "[75, 12000] loss: 0.697\n",
      "[76,  2000] loss: 0.532\n",
      "[76,  4000] loss: 0.558\n",
      "[76,  6000] loss: 0.593\n",
      "[76,  8000] loss: 0.669\n",
      "[76, 10000] loss: 0.619\n",
      "[76, 12000] loss: 0.638\n",
      "[77,  2000] loss: 0.550\n",
      "[77,  4000] loss: 0.591\n",
      "[77,  6000] loss: 0.675\n",
      "[77,  8000] loss: 0.615\n",
      "[77, 10000] loss: 0.630\n",
      "[77, 12000] loss: 0.688\n",
      "[78,  2000] loss: 0.522\n",
      "[78,  4000] loss: 0.596\n",
      "[78,  6000] loss: 0.615\n",
      "[78,  8000] loss: 0.605\n",
      "[78, 10000] loss: 0.616\n",
      "[78, 12000] loss: 0.664\n",
      "[79,  2000] loss: 0.595\n",
      "[79,  4000] loss: 0.598\n",
      "[79,  6000] loss: 0.595\n",
      "[79,  8000] loss: 0.638\n",
      "[79, 10000] loss: 0.657\n",
      "[79, 12000] loss: 0.676\n",
      "[80,  2000] loss: 0.584\n",
      "[80,  4000] loss: 0.599\n",
      "[80,  6000] loss: 0.597\n",
      "[80,  8000] loss: 0.625\n",
      "[80, 10000] loss: 0.622\n",
      "[80, 12000] loss: 0.659\n",
      "[81,  2000] loss: 0.595\n",
      "[81,  4000] loss: 0.605\n",
      "[81,  6000] loss: 0.634\n",
      "[81,  8000] loss: 0.613\n",
      "[81, 10000] loss: 0.648\n",
      "[81, 12000] loss: 0.646\n",
      "[82,  2000] loss: 0.550\n",
      "[82,  4000] loss: 0.589\n",
      "[82,  6000] loss: 0.579\n",
      "[82,  8000] loss: 0.614\n",
      "[82, 10000] loss: 0.613\n",
      "[82, 12000] loss: 0.655\n",
      "[83,  2000] loss: 0.556\n",
      "[83,  4000] loss: 0.601\n",
      "[83,  6000] loss: 0.622\n",
      "[83,  8000] loss: 0.592\n",
      "[83, 10000] loss: 0.628\n",
      "[83, 12000] loss: 0.700\n",
      "[84,  2000] loss: 0.562\n",
      "[84,  4000] loss: 0.597\n",
      "[84,  6000] loss: 0.600\n",
      "[84,  8000] loss: 0.633\n",
      "[84, 10000] loss: 0.666\n",
      "[84, 12000] loss: 0.648\n",
      "[85,  2000] loss: 0.586\n",
      "[85,  4000] loss: 0.585\n",
      "[85,  6000] loss: 0.618\n",
      "[85,  8000] loss: 0.642\n",
      "[85, 10000] loss: 0.650\n",
      "[85, 12000] loss: 0.629\n",
      "[86,  2000] loss: 0.544\n",
      "[86,  4000] loss: 0.604\n",
      "[86,  6000] loss: 0.644\n",
      "[86,  8000] loss: 0.643\n",
      "[86, 10000] loss: 0.644\n",
      "[86, 12000] loss: 0.684\n",
      "[87,  2000] loss: 0.549\n",
      "[87,  4000] loss: 0.618\n",
      "[87,  6000] loss: 0.626\n",
      "[87,  8000] loss: 0.644\n",
      "[87, 10000] loss: 0.665\n",
      "[87, 12000] loss: 0.645\n",
      "[88,  2000] loss: 0.565\n",
      "[88,  4000] loss: 0.601\n",
      "[88,  6000] loss: 0.650\n",
      "[88,  8000] loss: 0.626\n",
      "[88, 10000] loss: 0.667\n",
      "[88, 12000] loss: 0.647\n",
      "[89,  2000] loss: 0.573\n",
      "[89,  4000] loss: 0.608\n",
      "[89,  6000] loss: 0.661\n",
      "[89,  8000] loss: 0.653\n",
      "[89, 10000] loss: 0.637\n",
      "[89, 12000] loss: 0.694\n",
      "[90,  2000] loss: 0.603\n",
      "[90,  4000] loss: 0.611\n",
      "[90,  6000] loss: 0.605\n",
      "[90,  8000] loss: 0.629\n",
      "[90, 10000] loss: 0.709\n",
      "[90, 12000] loss: 0.671\n",
      "[91,  2000] loss: 0.611\n",
      "[91,  4000] loss: 0.579\n",
      "[91,  6000] loss: 0.660\n",
      "[91,  8000] loss: 0.670\n",
      "[91, 10000] loss: 0.655\n",
      "[91, 12000] loss: 0.669\n",
      "[92,  2000] loss: 0.545\n",
      "[92,  4000] loss: 0.628\n",
      "[92,  6000] loss: 0.594\n",
      "[92,  8000] loss: 0.619\n",
      "[92, 10000] loss: 0.689\n",
      "[92, 12000] loss: 0.738\n",
      "[93,  2000] loss: 0.523\n",
      "[93,  4000] loss: 0.601\n",
      "[93,  6000] loss: 0.593\n",
      "[93,  8000] loss: 0.619\n",
      "[93, 10000] loss: 0.658\n",
      "[93, 12000] loss: 0.689\n",
      "[94,  2000] loss: 0.611\n",
      "[94,  4000] loss: 0.683\n",
      "[94,  6000] loss: 0.642\n",
      "[94,  8000] loss: 0.650\n",
      "[94, 10000] loss: 0.685\n",
      "[94, 12000] loss: 0.700\n",
      "[95,  2000] loss: 0.623\n",
      "[95,  4000] loss: 0.566\n",
      "[95,  6000] loss: 0.666\n",
      "[95,  8000] loss: 0.657\n",
      "[95, 10000] loss: 0.674\n",
      "[95, 12000] loss: 0.662\n",
      "[96,  2000] loss: 0.594\n",
      "[96,  4000] loss: 0.656\n",
      "[96,  6000] loss: 0.641\n",
      "[96,  8000] loss: 0.705\n",
      "[96, 10000] loss: 0.663\n",
      "[96, 12000] loss: 0.749\n",
      "[97,  2000] loss: 0.645\n",
      "[97,  4000] loss: 0.608\n",
      "[97,  6000] loss: 0.637\n",
      "[97,  8000] loss: 0.688\n",
      "[97, 10000] loss: 0.667\n",
      "[97, 12000] loss: 0.661\n",
      "[98,  2000] loss: 0.594\n",
      "[98,  4000] loss: 0.617\n",
      "[98,  6000] loss: 0.687\n",
      "[98,  8000] loss: 0.657\n",
      "[98, 10000] loss: 0.717\n",
      "[98, 12000] loss: 0.697\n",
      "[99,  2000] loss: 0.591\n",
      "[99,  4000] loss: 0.606\n",
      "[99,  6000] loss: 0.632\n",
      "[99,  8000] loss: 0.619\n",
      "[99, 10000] loss: 0.667\n",
      "[99, 12000] loss: 0.692\n",
      "[100,  2000] loss: 0.603\n",
      "[100,  4000] loss: 0.609\n",
      "[100,  6000] loss: 0.615\n",
      "[100,  8000] loss: 0.605\n",
      "[100, 10000] loss: 0.655\n",
      "[100, 12000] loss: 0.676\n",
      "运行时间为： 3289.0089020729065 秒\n",
      "Finished Training\n"
     ]
    }
   ],
   "source": [
    "net = Net()\n",
    "# 定义损失函数和优化器\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(net.parameters(), lr=0.001, momentum=0.9)\n",
    "\n",
    "start_time = time.time()\n",
    "# 训练模型\n",
    "for epoch in range(100):  # 多次循环遍历数据集\n",
    "    running_loss = 0.0\n",
    "    for i, data in enumerate(trainloader, 0):\n",
    "        inputs, labels = data\n",
    "        optimizer.zero_grad()\n",
    "        outputs = net(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "        if i % 2000 == 1999:    # 每2000个小批量打印一次损失值\n",
    "            print('[%d, %5d] loss: %.3f' %\n",
    "                  (epoch + 1, i + 1, running_loss / 2000))\n",
    "            running_loss = 0.0\n",
    "\n",
    "end_time = time.time()\n",
    "total_time = end_time - start_time\n",
    "print(\"运行时间为：\", total_time, \"秒\")\n",
    "print('Finished Training')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1,  2000] loss: 2.264\n",
      "[1,  4000] loss: 1.922\n",
      "[1,  6000] loss: 1.671\n",
      "[1,  8000] loss: 1.612\n",
      "[1, 10000] loss: 1.534\n",
      "[1, 12000] loss: 1.492\n",
      "[2,  2000] loss: 1.406\n",
      "[2,  4000] loss: 1.385\n",
      "[2,  6000] loss: 1.348\n",
      "[2,  8000] loss: 1.333\n",
      "[2, 10000] loss: 1.311\n",
      "[2, 12000] loss: 1.266\n",
      "运行时间为： 131.9481692314148 秒\n",
      "Finished Training\n"
     ]
    }
   ],
   "source": [
    "# device = torch.device('cuda')\n",
    "# device = torch.device('mps')\n",
    "net = Net()\n",
    "# net = net.to(device)\n",
    "\n",
    "# 定义损失函数和优化器\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(net.parameters(), lr=0.001, momentum=0.9)\n",
    "\n",
    "start_time = time.time()\n",
    "# 训练模型\n",
    "for epoch in range(2):  # 多次循环遍历数据集\n",
    "    running_loss = 0.0\n",
    "    for i, data in enumerate(trainloader, 0):\n",
    "        inputs, labels = data\n",
    "        optimizer.zero_grad()\n",
    "        # outputs = net(inputs.to(device))\n",
    "        outputs = net(inputs)\n",
    "        # loss = criterion(outputs, labels.to(device))\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "        if i % 2000 == 1999:    # 每2000个小批量打印一次损失值\n",
    "            print('[%d, %5d] loss: %.3f' %\n",
    "                  (epoch + 1, i + 1, running_loss / 2000))\n",
    "            running_loss = 0.0\n",
    "\n",
    "end_time = time.time()\n",
    "total_time = end_time - start_time\n",
    "print(\"运行时间为：\", total_time, \"秒\")\n",
    "print('Finished Training')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "slow_conv2d_forward_mps: input(device='cpu') and weight(device=mps:0')  must be on the same device",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mRuntimeError\u001B[0m                              Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[9], line 7\u001B[0m\n\u001B[1;32m      5\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m data \u001B[38;5;129;01min\u001B[39;00m testloader:\n\u001B[1;32m      6\u001B[0m     images, labels \u001B[38;5;241m=\u001B[39m data\n\u001B[0;32m----> 7\u001B[0m     outputs \u001B[38;5;241m=\u001B[39m \u001B[43mnet\u001B[49m\u001B[43m(\u001B[49m\u001B[43mimages\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m      8\u001B[0m     _, predicted \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39mmax(outputs\u001B[38;5;241m.\u001B[39mdata, \u001B[38;5;241m1\u001B[39m)\n\u001B[1;32m      9\u001B[0m     total \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m labels\u001B[38;5;241m.\u001B[39msize(\u001B[38;5;241m0\u001B[39m)\n",
      "File \u001B[0;32m~/Desktop/ml/dlwpt-code/venv/lib/python3.9/site-packages/torch/nn/modules/module.py:1194\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *input, **kwargs)\u001B[0m\n\u001B[1;32m   1190\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[1;32m   1191\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[1;32m   1192\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[1;32m   1193\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[0;32m-> 1194\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1195\u001B[0m \u001B[38;5;66;03m# Do not call functions when jit is used\u001B[39;00m\n\u001B[1;32m   1196\u001B[0m full_backward_hooks, non_full_backward_hooks \u001B[38;5;241m=\u001B[39m [], []\n",
      "Cell \u001B[0;32mIn[3], line 13\u001B[0m, in \u001B[0;36mNet.forward\u001B[0;34m(self, x)\u001B[0m\n\u001B[1;32m     12\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mforward\u001B[39m(\u001B[38;5;28mself\u001B[39m, x):\n\u001B[0;32m---> 13\u001B[0m     x \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mpool(torch\u001B[38;5;241m.\u001B[39mrelu(\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mconv1\u001B[49m\u001B[43m(\u001B[49m\u001B[43mx\u001B[49m\u001B[43m)\u001B[49m))\n\u001B[1;32m     14\u001B[0m     x \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mpool(torch\u001B[38;5;241m.\u001B[39mrelu(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mconv2(x)))\n\u001B[1;32m     15\u001B[0m     x \u001B[38;5;241m=\u001B[39m x\u001B[38;5;241m.\u001B[39mview(\u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m, \u001B[38;5;241m16\u001B[39m \u001B[38;5;241m*\u001B[39m \u001B[38;5;241m5\u001B[39m \u001B[38;5;241m*\u001B[39m \u001B[38;5;241m5\u001B[39m)\n",
      "File \u001B[0;32m~/Desktop/ml/dlwpt-code/venv/lib/python3.9/site-packages/torch/nn/modules/module.py:1194\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *input, **kwargs)\u001B[0m\n\u001B[1;32m   1190\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[1;32m   1191\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[1;32m   1192\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[1;32m   1193\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[0;32m-> 1194\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1195\u001B[0m \u001B[38;5;66;03m# Do not call functions when jit is used\u001B[39;00m\n\u001B[1;32m   1196\u001B[0m full_backward_hooks, non_full_backward_hooks \u001B[38;5;241m=\u001B[39m [], []\n",
      "File \u001B[0;32m~/Desktop/ml/dlwpt-code/venv/lib/python3.9/site-packages/torch/nn/modules/conv.py:463\u001B[0m, in \u001B[0;36mConv2d.forward\u001B[0;34m(self, input)\u001B[0m\n\u001B[1;32m    462\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mforward\u001B[39m(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;28minput\u001B[39m: Tensor) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m Tensor:\n\u001B[0;32m--> 463\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_conv_forward\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mweight\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbias\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/Desktop/ml/dlwpt-code/venv/lib/python3.9/site-packages/torch/nn/modules/conv.py:459\u001B[0m, in \u001B[0;36mConv2d._conv_forward\u001B[0;34m(self, input, weight, bias)\u001B[0m\n\u001B[1;32m    455\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mpadding_mode \u001B[38;5;241m!=\u001B[39m \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mzeros\u001B[39m\u001B[38;5;124m'\u001B[39m:\n\u001B[1;32m    456\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m F\u001B[38;5;241m.\u001B[39mconv2d(F\u001B[38;5;241m.\u001B[39mpad(\u001B[38;5;28minput\u001B[39m, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_reversed_padding_repeated_twice, mode\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mpadding_mode),\n\u001B[1;32m    457\u001B[0m                     weight, bias, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mstride,\n\u001B[1;32m    458\u001B[0m                     _pair(\u001B[38;5;241m0\u001B[39m), \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdilation, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mgroups)\n\u001B[0;32m--> 459\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mF\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mconv2d\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mweight\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mbias\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mstride\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    460\u001B[0m \u001B[43m                \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mpadding\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdilation\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mgroups\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[0;31mRuntimeError\u001B[0m: slow_conv2d_forward_mps: input(device='cpu') and weight(device=mps:0')  must be on the same device"
     ]
    }
   ],
   "source": [
    "# 测试模型\n",
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():\n",
    "    for data in testloader:\n",
    "        images, labels = data\n",
    "        outputs = net(images)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "print('Accuracy of the network on the 10000 test images: %d %%' % (\n",
    "    100 * correct / total))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
